{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, LSTM, Flatten, Dense,, BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input and Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function takes in preprocessed data (classes balanced, duplicates removed, NOT normalized)\n",
    "The standard scaler has to be exported so it is important to normalize the data here\n",
    "\n",
    "Change the paths in trainFull and TestFull to desired preprocessedData\n",
    "\n",
    "\"\"\"\n",
    "def getPreprocessedData():\n",
    "    \n",
    "    #Concatenate positive and negative samples\n",
    "    trainFull = pd.read_csv(\"PrerocessedData/trainFull.csv\")\n",
    "    testFull = pd.read_csv(\"PrerocessedData/testFull.csv\")\n",
    "    \n",
    "\n",
    "    #Reduce features and extract labels\n",
    "    trainX = trainFull.iloc[:,:-2]\n",
    "    trainY = trainFull.iloc[:,-1]\n",
    "    testX = testFull.iloc[:,:-2]\n",
    "    testY = testFull.iloc[:,-1]\n",
    "    #Standard scale training data\n",
    "    trainScaler = StandardScaler()\n",
    "    trainScaler.fit(trainX)\n",
    "    trainX = trainScaler.transform(trainX)\n",
    "\n",
    "    #Export standard scale\n",
    "    #dump(trainScaler, 'standardScaler.bin', compress=True)\n",
    "\n",
    "    #Standard scale testing data\n",
    "    testScaler = StandardScaler()\n",
    "    testScaler.fit(testX)\n",
    "    testX = testScaler.transform(testX)\n",
    "\n",
    "    #Reshape data to 3D for CNN\n",
    "    trainX = trainX[..., None]\n",
    "    trainY = trainY[..., None]\n",
    "    testX = testX[..., None]\n",
    "    testY = testY[..., None]\n",
    "\n",
    "    return trainX, trainY, testX, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method contains the architecture for the CNN used.\n",
    "It uses tensorflow sequential as the basis to crease the model.\n",
    "\n",
    "@param numConvFilters: number of convolutional filters in the 2nd hidden layer. We recommend >32 to upscale.\n",
    "@param dropout: fraction of nodes dropped out of the Max Pooling layer.\n",
    "@param numDenseNodes: number of dense nodes in the feed forward layer.\n",
    "\n",
    "Recommended:\n",
    "numConvFilters = 242\n",
    "dropout = .215\n",
    "numDenseNodes = 190\n",
    "\n",
    "\"\"\"\n",
    "def newModel(numConvFilters = 242, dropout = .215, numDenseNodes = 190):\n",
    "    #Create new sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    #Add 1D Convolutional layers\n",
    "    \n",
    "    model.add((Conv1D(filters=32, kernel_size=3, activation='relu')))\n",
    "    model.add((Conv1D(filters=numConvFilters, kernel_size=3, activation='relu')))\n",
    "    #Add Max Pooling layer\n",
    "    model.add((MaxPooling1D(pool_size=2)))\n",
    "    #Apply dropout\n",
    "    model.add(Dropout(dropout))\n",
    "    #Flatten model\n",
    "    model.add(Flatten())\n",
    "    #Add fully connected dense layer\n",
    "    model.add(Dense(numDenseNodes, activation='sigmoid'))\n",
    "    #Add output node\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "    \n",
    "    #Define optimizer\n",
    "    adam = tf.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #Compile model\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(curve='PR'),                                                                                         tf.keras.metrics.PrecisionAtRecall(0.8)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getPreprocessedData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fr/03zv9zcj7j146mrr4jsb58kw0000gn/T/ipykernel_61481/855137266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPreprocessedData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getPreprocessedData' is not defined"
     ]
    }
   ],
   "source": [
    "trainX, trainY, testX, testY = getPreprocessedData()\n",
    "model = newModel()\n",
    "model.fit(trainX, trainY, epochs = 30, batch_size=20, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create confusion Matrix\n",
    "def confusionMatrix(model, testX, testY):\n",
    "    pred = model.predict(testX)\n",
    "    predy = []\n",
    "    for i in range(0,len(pred)):\n",
    "        if pred[i] >= .5:\n",
    "            predy.append(1)\n",
    "        else:\n",
    "            predy.append(0)\n",
    "        \n",
    "    print(confusion_matrix(testY,predy))\n",
    "    \n",
    "#Calculates our model metrics\n",
    "def modelMetrics(testX, testY, model):\n",
    "    y_scores = model.predict(testX)\n",
    "    precision, recall, thresholds = precision_recall_curve(testY, y_scores)\n",
    "    rec80 = np.max(recall[precision >= .8])\n",
    "    print(\"Recall at 80% Precision: \" +str(rec80))\n",
    "    aucPR = auc(recall, precision)\n",
    "    print(\"PR-AUC: \" + str(aucPR))\n",
    "    \n",
    "    return rec80, aucPR, \n",
    "\n",
    "#plots a PR curve\n",
    "def PRplot(testX, testY, model):\n",
    "    y_scores = model.predict(testX)\n",
    "    precision, recall, thresholds = precision_recall_curve(testY, y_scores)\n",
    "    plt.plot(recall, precision)\n",
    "    plt.title(\"PR curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMetrics(testX, testY, model)\n",
    "confusionMatrix(model, testX, testY)\n",
    "PRplot(testX, testY, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
